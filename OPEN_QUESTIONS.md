# Open Questions — Taxonomy Experiment

- **Generated by:** Claude Opus 4.6
- **Timestamp:** 2026-02-14
- **Context:** Experiment deriving a hierarchical taxonomy from `scenes/` directory only

---

## Open Questions

### OQ-1: Can the system detect when its derived taxonomy no longer matches its declared vision?

- **Raised by:** Dan
- **Date:** 2026-02-14
- **Status:** Open
- **Why it matters:** The North Star and Design Charter exist as explicit governance anchors, but if nothing checks the graph against them, they become decorative rather than functional. The system could silently regress into a note-taking system — the exact outcome the Design Charter prohibits.

---

## Journal

### 2026-02-14 — Claude Opus 4.6 — Initial analysis

**Short answer: Not yet. The system has drift detection, but it's all structural, not vision-aligned.**

#### What the system CAN detect (structural drift)

- **Spec drift tests** — verify required sections present, filename regex conformance, deterministic rendering, repeat-run stability. These check *"is this spec well-formed?"*, not *"is this spec serving the North Star?"*
- **Prompt lineage drift tests** — verify deterministic materialization (same base + same deltas = byte-identical output). Checks replay fidelity, not semantic direction.
- **Session index rebuild rules** — detect when indexes fall out of sync with artifact directories. Structural bookkeeping.
- **Kalshi phase history `drift_signals`** — tracks regression, oscillation, stagnation within a single project's phase progression. Closest thing to semantic drift detection, but scoped to one project and manually authored.
- **Robustness gate directives** — perturbation tiers, invariance requirements, promotion criteria. Designed for pipeline/model robustness, not taxonomy-vision coherence.

#### What the system CANNOT detect

Whether the taxonomy as a whole is drifting from the declared vision:

> *"ideas become structured artifacts, artifacts become connected meaning, and meaning becomes executable reasoning"*

No scene defines a mechanism that periodically compares the graph's actual content distribution, edge density, or conceptual coverage against the North Star or Design Charter. The system can tell you if a spec is malformed, but not if the entire knowledge graph is quietly becoming the "note-taking system" the Design Charter explicitly says it is not.

#### The specific gap

There is no **vision-alignment audit** — something that would ask:

1. Are new nodes compounding into connected meaning, or accumulating as isolated artifacts?
2. Is "executable reasoning" actually emerging (agents traversing the graph), or is the graph inert?
3. Are principles governing pattern selection, or are patterns proliferating without principled constraint?
4. What percentage of the graph is reachable from the North Star via typed edges vs. orphaned?

#### Closest existing precedent

The `drift_signals` object in `kalshi_phase_history.scene.json` (fields: `regression`, `oscillation`, `stagnation`) is the nearest structural analog. It would need to be generalized to the whole taxonomy and grounded against the declared vision rather than a single project's phase sequence.
